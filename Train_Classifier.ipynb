{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayakpaul/Denoised-Smoothing-TF/blob/main/Train_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T18oVtwdDYtM"
   },
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quAlaCqeaJJm",
    "outputId": "afa2e8c6-229c-4dcc-910a-0ba596f15cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User name: sayakpaul\n",
      "Password: ··········\n",
      "Repo Address: sayakpaul/Denoised-Smoothing-TF\n",
      "Branch name: main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "from getpass import getpass\n",
    "\n",
    "user = input('User name: ')\n",
    "password = getpass('Password: ')\n",
    "password = urllib.parse.quote(password)\n",
    "repo_address = input('Repo Address: ')\n",
    "branch_name = input('Branch name: ')\n",
    "\n",
    "cmd_string = 'git clone https://{}:{}@github.com/{}.git -b {}'.format(\n",
    "    user, password, repo_address, branch_name\n",
    ")\n",
    "\n",
    "os.system(cmd_string)\n",
    "cmd_string, password = \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hvge9_rAA9tj"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Denoised-Smoothing-TF\")\n",
    "\n",
    "from models import resnet20\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTxobBFVFRSN",
    "outputId": "ec47e51c-4051-4046-b8b3-37a161cfeb24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Number of accelerators:  4\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    tpu = None\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError: \n",
    "    strategy = tf.distribute.MirroredStrategy() \n",
    "\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nJS4plRSGg9e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPXES8gJDbAA"
   },
   "source": [
    "## Load the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PeEXTDzCBuFr",
    "outputId": "87e05713-4ebc-4839-d20f-58abf254549e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 50000\n",
      "Total test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"Total training examples: {len(x_train)}\")\n",
    "print(f\"Total test examples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4RfWSOZDfS6"
   },
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uRL0hOAUByGr"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
    "EPOCHS = 200 \n",
    "START_LR = 0.1 \n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DUjJliJDiEq"
   },
   "source": [
    "## Prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tQT1v91WB0Cm"
   },
   "outputs": [],
   "source": [
    "# Augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomCrop(32, 32),\n",
    "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "        layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now, map the augmentation pipeline to our training dataset\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y), \n",
    "         num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y), \n",
    "         num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# Compute the mean and the variance of the training data for normalization\n",
    "data_augmentation.layers[0].adapt(x_train/255.) # Notice the scaling step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uKnDkElDmPK"
   },
   "source": [
    "## Model utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aVWCFj3AB4Km"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < int(EPOCHS * 0.25) - 1:\n",
    "        return START_LR\n",
    "    elif epoch < int(EPOCHS*0.5) -1:\n",
    "        return float(START_LR * 0.1)\n",
    "    elif epoch < int(EPOCHS*0.75) -1:\n",
    "        return float(START_LR * 0.01)\n",
    "    else:\n",
    "        return float(START_LR * 0.001)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JfMwi6acCGxE"
   },
   "outputs": [],
   "source": [
    "def get_model(n_classes=10):\n",
    "    n = 2\n",
    "    depth = n * 9 + 2\n",
    "    n_blocks = ((depth - 2) // 9) - 1\n",
    "\n",
    "    # The input tensor\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = data_augmentation(inputs) # Normalize and augment\n",
    "\n",
    "    # The Stem Convolution Group\n",
    "    x = resnet20.stem(x)\n",
    "\n",
    "    # The learner\n",
    "    x = resnet20.learner(x, n_blocks)\n",
    "\n",
    "    # The Classifier for 10 classes\n",
    "    outputs = resnet20.classifier(x, 10)\n",
    "\n",
    "    # Instantiate the Model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k38CSL2RCgsb",
    "outputId": "2e2b5e4e-49d6-4596-db04-5cbdcea28bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Assets written to: gs://denoised-smoothing-tf/initial_model_resnet20/assets\n"
     ]
    }
   ],
   "source": [
    "# Serialize the initial model for better reproducibility\n",
    "with strategy.scope():\n",
    "    get_model().save(\"gs://denoised-smoothing-tf/initial_model_resnet20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLjm5gx4Dp1n"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BwGi5W1tB6N9"
   },
   "outputs": [],
   "source": [
    "# Optimizer and loss function.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=START_LR, momentum=0.9)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "ZyuPYAniB8oo",
    "outputId": "624abaa8-0e3f-455c-9a7a-849dd84ccec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "INFO:tensorflow:batch_all_reduce: 64 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 64 all-reduces with algorithm = nccl, num_packs = 1\n",
      "98/98 [==============================] - 48s 77ms/step - loss: 2.7407 - accuracy: 0.3050 - val_loss: 3.4464 - val_accuracy: 0.1739\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 2.0492 - accuracy: 0.5223 - val_loss: 2.6184 - val_accuracy: 0.3323\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 1.7267 - accuracy: 0.6042 - val_loss: 2.8323 - val_accuracy: 0.3219\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 1.5095 - accuracy: 0.6541 - val_loss: 2.2621 - val_accuracy: 0.4457\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 5s 44ms/step - loss: 1.3449 - accuracy: 0.6920 - val_loss: 2.0629 - val_accuracy: 0.5183\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 1.2148 - accuracy: 0.7227 - val_loss: 1.8693 - val_accuracy: 0.5308\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 1.1136 - accuracy: 0.7437 - val_loss: 1.4404 - val_accuracy: 0.6366\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 1.0296 - accuracy: 0.7628 - val_loss: 2.2735 - val_accuracy: 0.5020\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.9802 - accuracy: 0.7686 - val_loss: 1.2475 - val_accuracy: 0.6704\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.9354 - accuracy: 0.7807 - val_loss: 1.5000 - val_accuracy: 0.5783\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.9057 - accuracy: 0.7828 - val_loss: 1.2100 - val_accuracy: 0.6842\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.8621 - accuracy: 0.7954 - val_loss: 1.2761 - val_accuracy: 0.6822\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.8317 - accuracy: 0.8033 - val_loss: 1.1675 - val_accuracy: 0.6924\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.8093 - accuracy: 0.8085 - val_loss: 1.0958 - val_accuracy: 0.7149\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.8014 - accuracy: 0.8060 - val_loss: 1.2880 - val_accuracy: 0.6759\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7899 - accuracy: 0.8104 - val_loss: 1.7088 - val_accuracy: 0.6021\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7534 - accuracy: 0.8247 - val_loss: 1.1778 - val_accuracy: 0.7051\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7648 - accuracy: 0.8153 - val_loss: 1.0101 - val_accuracy: 0.7382\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7580 - accuracy: 0.8195 - val_loss: 1.1301 - val_accuracy: 0.7152\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7446 - accuracy: 0.8226 - val_loss: 1.0809 - val_accuracy: 0.7233\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7392 - accuracy: 0.8270 - val_loss: 1.1445 - val_accuracy: 0.7010\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7139 - accuracy: 0.8337 - val_loss: 1.1427 - val_accuracy: 0.7144\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7181 - accuracy: 0.8323 - val_loss: 1.1203 - val_accuracy: 0.7143\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7098 - accuracy: 0.8367 - val_loss: 0.8931 - val_accuracy: 0.7771\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6995 - accuracy: 0.8406 - val_loss: 1.0735 - val_accuracy: 0.7230\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6944 - accuracy: 0.8404 - val_loss: 1.4688 - val_accuracy: 0.6591\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7001 - accuracy: 0.8404 - val_loss: 1.0767 - val_accuracy: 0.7142\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6811 - accuracy: 0.8483 - val_loss: 1.3551 - val_accuracy: 0.6994\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6860 - accuracy: 0.8468 - val_loss: 0.9022 - val_accuracy: 0.7779\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6732 - accuracy: 0.8500 - val_loss: 1.1234 - val_accuracy: 0.7320\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6737 - accuracy: 0.8532 - val_loss: 1.0400 - val_accuracy: 0.7438\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6849 - accuracy: 0.8482 - val_loss: 1.6121 - val_accuracy: 0.6483\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6655 - accuracy: 0.8545 - val_loss: 1.6324 - val_accuracy: 0.6474\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.6614 - accuracy: 0.8564 - val_loss: 1.1153 - val_accuracy: 0.7273\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6625 - accuracy: 0.8550 - val_loss: 1.1573 - val_accuracy: 0.7279\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6592 - accuracy: 0.8582 - val_loss: 1.0030 - val_accuracy: 0.7639\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6543 - accuracy: 0.8564 - val_loss: 0.8299 - val_accuracy: 0.8002\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6613 - accuracy: 0.8570 - val_loss: 0.8968 - val_accuracy: 0.7884\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6546 - accuracy: 0.8593 - val_loss: 1.6528 - val_accuracy: 0.6422\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 0.6585 - accuracy: 0.8606 - val_loss: 1.0651 - val_accuracy: 0.7465\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6431 - accuracy: 0.8651 - val_loss: 1.4029 - val_accuracy: 0.6693\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6545 - accuracy: 0.8606 - val_loss: 1.2405 - val_accuracy: 0.7232\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6471 - accuracy: 0.8651 - val_loss: 1.1707 - val_accuracy: 0.7181\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6476 - accuracy: 0.8637 - val_loss: 1.0128 - val_accuracy: 0.7696\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6334 - accuracy: 0.8707 - val_loss: 0.8794 - val_accuracy: 0.7934\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6380 - accuracy: 0.8679 - val_loss: 1.1109 - val_accuracy: 0.7366\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.6416 - accuracy: 0.8665 - val_loss: 1.2326 - val_accuracy: 0.7142\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.6388 - accuracy: 0.8683 - val_loss: 1.0889 - val_accuracy: 0.7447\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6416 - accuracy: 0.8675 - val_loss: 1.0694 - val_accuracy: 0.7431\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.6859 - accuracy: 0.8514 - val_loss: 0.6428 - val_accuracy: 0.8670\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.5113 - accuracy: 0.9140 - val_loss: 0.6210 - val_accuracy: 0.8740\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.4869 - accuracy: 0.9222 - val_loss: 0.6160 - val_accuracy: 0.8737\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.4668 - accuracy: 0.9256 - val_loss: 0.5954 - val_accuracy: 0.8792\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.4539 - accuracy: 0.9273 - val_loss: 0.5974 - val_accuracy: 0.8779\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.4379 - accuracy: 0.9318 - val_loss: 0.5977 - val_accuracy: 0.8746\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.4229 - accuracy: 0.9344 - val_loss: 0.5584 - val_accuracy: 0.8882\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.4141 - accuracy: 0.9357 - val_loss: 0.5517 - val_accuracy: 0.8907\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.4104 - accuracy: 0.9373 - val_loss: 0.5606 - val_accuracy: 0.8852\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3970 - accuracy: 0.9380 - val_loss: 0.5745 - val_accuracy: 0.8794\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3928 - accuracy: 0.9404 - val_loss: 0.5305 - val_accuracy: 0.8954\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3804 - accuracy: 0.9438 - val_loss: 0.5416 - val_accuracy: 0.8929\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3751 - accuracy: 0.9437 - val_loss: 0.5707 - val_accuracy: 0.8824\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3685 - accuracy: 0.9461 - val_loss: 0.5522 - val_accuracy: 0.8843\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3637 - accuracy: 0.9454 - val_loss: 0.5442 - val_accuracy: 0.8900\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3527 - accuracy: 0.9482 - val_loss: 0.5339 - val_accuracy: 0.8886\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3446 - accuracy: 0.9508 - val_loss: 0.5397 - val_accuracy: 0.8846\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3442 - accuracy: 0.9487 - val_loss: 0.5312 - val_accuracy: 0.8890\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3374 - accuracy: 0.9492 - val_loss: 0.5499 - val_accuracy: 0.8840\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3258 - accuracy: 0.9534 - val_loss: 0.5783 - val_accuracy: 0.8761\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3173 - accuracy: 0.9547 - val_loss: 0.5573 - val_accuracy: 0.8803\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3174 - accuracy: 0.9555 - val_loss: 0.5848 - val_accuracy: 0.8753\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3167 - accuracy: 0.9525 - val_loss: 0.5330 - val_accuracy: 0.8911\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3123 - accuracy: 0.9534 - val_loss: 0.5794 - val_accuracy: 0.8773\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3006 - accuracy: 0.9564 - val_loss: 0.5466 - val_accuracy: 0.8790\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.3005 - accuracy: 0.9566 - val_loss: 0.5368 - val_accuracy: 0.8853\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 0.2949 - accuracy: 0.9574 - val_loss: 0.5807 - val_accuracy: 0.8718\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2949 - accuracy: 0.9566 - val_loss: 0.5611 - val_accuracy: 0.8778\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2897 - accuracy: 0.9580 - val_loss: 0.5374 - val_accuracy: 0.8850\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2872 - accuracy: 0.9573 - val_loss: 0.5012 - val_accuracy: 0.8953\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2885 - accuracy: 0.9547 - val_loss: 0.5620 - val_accuracy: 0.8787\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2808 - accuracy: 0.9574 - val_loss: 0.5560 - val_accuracy: 0.8761\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2770 - accuracy: 0.9586 - val_loss: 0.5162 - val_accuracy: 0.8885\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2767 - accuracy: 0.9592 - val_loss: 0.5400 - val_accuracy: 0.8841\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2760 - accuracy: 0.9580 - val_loss: 0.6082 - val_accuracy: 0.8681\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2649 - accuracy: 0.9621 - val_loss: 0.5311 - val_accuracy: 0.8809\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2653 - accuracy: 0.9615 - val_loss: 0.6548 - val_accuracy: 0.8560\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2635 - accuracy: 0.9612 - val_loss: 0.5269 - val_accuracy: 0.8809\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2622 - accuracy: 0.9610 - val_loss: 0.4805 - val_accuracy: 0.8972\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.2649 - accuracy: 0.9606 - val_loss: 0.6164 - val_accuracy: 0.8667\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2544 - accuracy: 0.9618 - val_loss: 0.5171 - val_accuracy: 0.8894\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2531 - accuracy: 0.9612 - val_loss: 0.5363 - val_accuracy: 0.8817\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2596 - accuracy: 0.9585 - val_loss: 0.5058 - val_accuracy: 0.8905\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2539 - accuracy: 0.9614 - val_loss: 0.5897 - val_accuracy: 0.8655\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2503 - accuracy: 0.9624 - val_loss: 0.5230 - val_accuracy: 0.8847\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2498 - accuracy: 0.9603 - val_loss: 0.5430 - val_accuracy: 0.8798\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2496 - accuracy: 0.9616 - val_loss: 0.5172 - val_accuracy: 0.8835\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2496 - accuracy: 0.9599 - val_loss: 0.6358 - val_accuracy: 0.8573\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.2524 - accuracy: 0.9589 - val_loss: 0.5633 - val_accuracy: 0.8766\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2438 - accuracy: 0.9631 - val_loss: 0.5549 - val_accuracy: 0.8725\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2501 - accuracy: 0.9594 - val_loss: 0.4509 - val_accuracy: 0.9014\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2158 - accuracy: 0.9728 - val_loss: 0.4469 - val_accuracy: 0.9047\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2082 - accuracy: 0.9765 - val_loss: 0.4470 - val_accuracy: 0.9058\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2044 - accuracy: 0.9779 - val_loss: 0.4543 - val_accuracy: 0.9025\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2014 - accuracy: 0.9788 - val_loss: 0.4409 - val_accuracy: 0.9095\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.2001 - accuracy: 0.9790 - val_loss: 0.4386 - val_accuracy: 0.9077\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1951 - accuracy: 0.9820 - val_loss: 0.4461 - val_accuracy: 0.9053\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1967 - accuracy: 0.9798 - val_loss: 0.4614 - val_accuracy: 0.9007\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1934 - accuracy: 0.9812 - val_loss: 0.4523 - val_accuracy: 0.9031\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1947 - accuracy: 0.9807 - val_loss: 0.4512 - val_accuracy: 0.9039\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1910 - accuracy: 0.9812 - val_loss: 0.4540 - val_accuracy: 0.9050\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 0.1864 - accuracy: 0.9840 - val_loss: 0.4546 - val_accuracy: 0.9044\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1877 - accuracy: 0.9831 - val_loss: 0.4487 - val_accuracy: 0.9075\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1876 - accuracy: 0.9828 - val_loss: 0.4610 - val_accuracy: 0.9041\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1860 - accuracy: 0.9839 - val_loss: 0.4510 - val_accuracy: 0.9071\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1858 - accuracy: 0.9834 - val_loss: 0.4566 - val_accuracy: 0.9043\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1854 - accuracy: 0.9836 - val_loss: 0.4471 - val_accuracy: 0.9065\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1821 - accuracy: 0.9847 - val_loss: 0.4602 - val_accuracy: 0.9034\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1863 - accuracy: 0.9833 - val_loss: 0.4527 - val_accuracy: 0.9053\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1831 - accuracy: 0.9840 - val_loss: 0.4477 - val_accuracy: 0.9069\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1829 - accuracy: 0.9833 - val_loss: 0.4533 - val_accuracy: 0.9048\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.1809 - accuracy: 0.9844 - val_loss: 0.4497 - val_accuracy: 0.9061\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.1797 - accuracy: 0.9855 - val_loss: 0.4450 - val_accuracy: 0.9073\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1813 - accuracy: 0.9848 - val_loss: 0.4556 - val_accuracy: 0.9040\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1805 - accuracy: 0.9845 - val_loss: 0.4545 - val_accuracy: 0.9051\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1760 - accuracy: 0.9870 - val_loss: 0.4525 - val_accuracy: 0.9047\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1771 - accuracy: 0.9851 - val_loss: 0.4508 - val_accuracy: 0.9057\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1775 - accuracy: 0.9853 - val_loss: 0.4577 - val_accuracy: 0.9041\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1775 - accuracy: 0.9850 - val_loss: 0.4527 - val_accuracy: 0.9069\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1744 - accuracy: 0.9870 - val_loss: 0.4542 - val_accuracy: 0.9057\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1759 - accuracy: 0.9858 - val_loss: 0.4592 - val_accuracy: 0.9036\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1722 - accuracy: 0.9869 - val_loss: 0.4531 - val_accuracy: 0.9048\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1740 - accuracy: 0.9862 - val_loss: 0.4571 - val_accuracy: 0.9041\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1722 - accuracy: 0.9868 - val_loss: 0.4640 - val_accuracy: 0.9011\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1738 - accuracy: 0.9858 - val_loss: 0.4559 - val_accuracy: 0.9039\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1708 - accuracy: 0.9872 - val_loss: 0.4643 - val_accuracy: 0.9032\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1739 - accuracy: 0.9854 - val_loss: 0.4579 - val_accuracy: 0.9037\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1728 - accuracy: 0.9855 - val_loss: 0.4559 - val_accuracy: 0.9033\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1717 - accuracy: 0.9861 - val_loss: 0.4517 - val_accuracy: 0.9055\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1707 - accuracy: 0.9867 - val_loss: 0.4679 - val_accuracy: 0.9020\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1693 - accuracy: 0.9867 - val_loss: 0.4524 - val_accuracy: 0.9061\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1685 - accuracy: 0.9874 - val_loss: 0.4624 - val_accuracy: 0.9020\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1685 - accuracy: 0.9869 - val_loss: 0.4519 - val_accuracy: 0.9037\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1679 - accuracy: 0.9869 - val_loss: 0.4507 - val_accuracy: 0.9053\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1658 - accuracy: 0.9873 - val_loss: 0.4584 - val_accuracy: 0.9027\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1657 - accuracy: 0.9878 - val_loss: 0.4604 - val_accuracy: 0.9028\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1657 - accuracy: 0.9870 - val_loss: 0.4622 - val_accuracy: 0.9039\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 0.1659 - accuracy: 0.9885 - val_loss: 0.4560 - val_accuracy: 0.9061\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1657 - accuracy: 0.9870 - val_loss: 0.4546 - val_accuracy: 0.9056\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1663 - accuracy: 0.9873 - val_loss: 0.4474 - val_accuracy: 0.9084\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1653 - accuracy: 0.9876 - val_loss: 0.4526 - val_accuracy: 0.9065\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1645 - accuracy: 0.9878 - val_loss: 0.4541 - val_accuracy: 0.9062\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1657 - accuracy: 0.9875 - val_loss: 0.4544 - val_accuracy: 0.9053\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1617 - accuracy: 0.9887 - val_loss: 0.4536 - val_accuracy: 0.9059\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1637 - accuracy: 0.9882 - val_loss: 0.4535 - val_accuracy: 0.9060\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1621 - accuracy: 0.9892 - val_loss: 0.4554 - val_accuracy: 0.9065\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1647 - accuracy: 0.9877 - val_loss: 0.4553 - val_accuracy: 0.9057\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1622 - accuracy: 0.9886 - val_loss: 0.4546 - val_accuracy: 0.9052\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1610 - accuracy: 0.9900 - val_loss: 0.4550 - val_accuracy: 0.9058\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1612 - accuracy: 0.9894 - val_loss: 0.4544 - val_accuracy: 0.9058\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1612 - accuracy: 0.9892 - val_loss: 0.4541 - val_accuracy: 0.9054\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1640 - accuracy: 0.9884 - val_loss: 0.4544 - val_accuracy: 0.9059\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1616 - accuracy: 0.9887 - val_loss: 0.4543 - val_accuracy: 0.9060\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1620 - accuracy: 0.9893 - val_loss: 0.4535 - val_accuracy: 0.9062\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1635 - accuracy: 0.9874 - val_loss: 0.4534 - val_accuracy: 0.9059\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1625 - accuracy: 0.9888 - val_loss: 0.4547 - val_accuracy: 0.9056\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1605 - accuracy: 0.9897 - val_loss: 0.4559 - val_accuracy: 0.9056\n",
      "Epoch 167/200\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1625 - accuracy: 0.9892 - val_loss: 0.4549 - val_accuracy: 0.9054\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1638 - accuracy: 0.9883 - val_loss: 0.4549 - val_accuracy: 0.9058\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1636 - accuracy: 0.9881 - val_loss: 0.4528 - val_accuracy: 0.9065\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1629 - accuracy: 0.9885 - val_loss: 0.4542 - val_accuracy: 0.9054\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1622 - accuracy: 0.9875 - val_loss: 0.4556 - val_accuracy: 0.9059\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.1600 - accuracy: 0.9891 - val_loss: 0.4544 - val_accuracy: 0.9062\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.1591 - accuracy: 0.9889 - val_loss: 0.4556 - val_accuracy: 0.9050\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1596 - accuracy: 0.9893 - val_loss: 0.4553 - val_accuracy: 0.9051\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1600 - accuracy: 0.9893 - val_loss: 0.4551 - val_accuracy: 0.9049\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1624 - accuracy: 0.9879 - val_loss: 0.4547 - val_accuracy: 0.9050\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1600 - accuracy: 0.9898 - val_loss: 0.4540 - val_accuracy: 0.9055\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1609 - accuracy: 0.9890 - val_loss: 0.4525 - val_accuracy: 0.9063\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1600 - accuracy: 0.9894 - val_loss: 0.4529 - val_accuracy: 0.9056\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1617 - accuracy: 0.9881 - val_loss: 0.4549 - val_accuracy: 0.9058\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1607 - accuracy: 0.9891 - val_loss: 0.4532 - val_accuracy: 0.9059\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1595 - accuracy: 0.9893 - val_loss: 0.4535 - val_accuracy: 0.9063\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 0.1614 - accuracy: 0.9888 - val_loss: 0.4541 - val_accuracy: 0.9057\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1609 - accuracy: 0.9888 - val_loss: 0.4545 - val_accuracy: 0.9053\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1612 - accuracy: 0.9889 - val_loss: 0.4546 - val_accuracy: 0.9049\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1602 - accuracy: 0.9897 - val_loss: 0.4541 - val_accuracy: 0.9056\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1603 - accuracy: 0.9890 - val_loss: 0.4531 - val_accuracy: 0.9063\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1604 - accuracy: 0.9895 - val_loss: 0.4553 - val_accuracy: 0.9054\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1622 - accuracy: 0.9892 - val_loss: 0.4542 - val_accuracy: 0.9056\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1601 - accuracy: 0.9887 - val_loss: 0.4535 - val_accuracy: 0.9052\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1596 - accuracy: 0.9892 - val_loss: 0.4543 - val_accuracy: 0.9058\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1595 - accuracy: 0.9896 - val_loss: 0.4528 - val_accuracy: 0.9059\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1611 - accuracy: 0.9884 - val_loss: 0.4545 - val_accuracy: 0.9059\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1597 - accuracy: 0.9897 - val_loss: 0.4554 - val_accuracy: 0.9052\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1593 - accuracy: 0.9899 - val_loss: 0.4545 - val_accuracy: 0.9056\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1592 - accuracy: 0.9896 - val_loss: 0.4549 - val_accuracy: 0.9062\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.1595 - accuracy: 0.9891 - val_loss: 0.4535 - val_accuracy: 0.9063\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1600 - accuracy: 0.9899 - val_loss: 0.4543 - val_accuracy: 0.9060\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1592 - accuracy: 0.9892 - val_loss: 0.4530 - val_accuracy: 0.9065\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.1589 - accuracy: 0.9894 - val_loss: 0.4530 - val_accuracy: 0.9064\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    rn_model = tf.keras.models.load_model(\"gs://denoised-smoothing-tf/initial_model_resnet20\")\n",
    "    rn_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history = rn_model.fit(train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "v1JWLrxYDFZ3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://denoised-smoothing-tf/resnet20_classifier/assets\n"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"test loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "rn_model.save(\"gs://denoised-smoothing-tf/resnet20_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "C6BohaLaEroC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.02%\n",
      "Test accuracy: 90.64%\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    _, train_acc = rn_model.evaluate(train_ds, verbose=0)\n",
    "    _, test_acc = rn_model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOTBCjLZ8eG46OboA5hr1WN",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Train_Classifier.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
