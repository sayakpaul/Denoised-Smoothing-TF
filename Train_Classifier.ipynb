{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Denoiser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMEJBLeHyZM7erGMVLj5Ltt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/Denoised-Smoothing-TF/blob/main/Train_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T18oVtwdDYtM"
      },
      "source": [
        "## Setup and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quAlaCqeaJJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f7ef90-1558-44dd-96c0-ced60d069ad3"
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "from getpass import getpass\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password)\n",
        "repo_address = input('Repo Address: ')\n",
        "branch_name = input('Branch name: ')\n",
        "\n",
        "cmd_string = 'git clone https://{}:{}@github.com/{}.git -b {}'.format(\n",
        "    user, password, repo_address, branch_name\n",
        ")\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: sayakpaul\n",
            "Password: ··········\n",
            "Repo Address: sayakpaul/Denoised-Smoothing-TF\n",
            "Branch name: main\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvge9_rAA9tj"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"Denoised-Smoothing-TF\")\n",
        "\n",
        "from models import resnet20\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTxobBFVFRSN",
        "outputId": "e37438b9-a9e0-4394-e5f4-7ce818d91d8e"
      },
      "source": [
        "try: \n",
        "    tpu = None\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: \n",
        "    strategy = tf.distribute.MirroredStrategy() \n",
        "\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.70.187.10:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.70.187.10:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of accelerators:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJS4plRSGg9e"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPXES8gJDbAA"
      },
      "source": [
        "## Load the CIFAR-10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeEXTDzCBuFr",
        "outputId": "9be88046-5fda-4853-e5f8-ec820c061ab1"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "print(f\"Total training examples: {len(x_train)}\")\n",
        "print(f\"Total test examples: {len(x_test)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training examples: 50000\n",
            "Total test examples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4RfWSOZDfS6"
      },
      "source": [
        "## Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRL0hOAUByGr"
      },
      "source": [
        "BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
        "EPOCHS = 200 \n",
        "START_LR = 0.1 \n",
        "AUTO = tf.data.AUTOTUNE"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DUjJliJDiEq"
      },
      "source": [
        "## Prepare data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQT1v91WB0Cm"
      },
      "source": [
        "# Augmentation pipeline\n",
        "simple_aug = tf.keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
        "        layers.experimental.preprocessing.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Now, map the augmentation pipeline to our training dataset\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(BATCH_SIZE * 100)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(lambda x, y: (simple_aug(x), y), num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "# Test dataset\n",
        "test_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uKnDkElDmPK"
      },
      "source": [
        "## Model utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVWCFj3AB4Km"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    if epoch < int(EPOCHS * 0.25) - 1:\n",
        "        return START_LR\n",
        "    elif epoch < int(EPOCHS*0.5) -1:\n",
        "        return float(START_LR * 0.1)\n",
        "    elif epoch < int(EPOCHS*0.75) -1:\n",
        "        return float(START_LR * 0.01)\n",
        "    else:\n",
        "        return float(START_LR * 0.001)\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch), verbose=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfMwi6acCGxE"
      },
      "source": [
        "def get_model(n_classes=10):\n",
        "    n = 2\n",
        "    depth = n * 9 + 2\n",
        "    n_blocks = ((depth - 2) // 9) - 1\n",
        "\n",
        "    # The input tensor\n",
        "    inputs = layers.Input(shape=(32, 32, 3))\n",
        "    x = layers.experimental.preprocessing.Rescaling(scale=1.0 / 127.5, offset=-1)(\n",
        "        inputs\n",
        "    )\n",
        "\n",
        "    # The Stem Convolution Group\n",
        "    x = resnet20.stem(x)\n",
        "\n",
        "    # The learner\n",
        "    x = resnet20.learner(x, n_blocks)\n",
        "\n",
        "    # The Classifier for 10 classes\n",
        "    outputs = resnet20.classifier(x, 10)\n",
        "\n",
        "    # Instantiate the Model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k38CSL2RCgsb",
        "outputId": "5f9fd15b-eace-48ad-c507-fc52cc2a877d"
      },
      "source": [
        "# Serialize the initial model for better reproducibility\n",
        "with strategy.scope():\n",
        "    get_model().save(\"gs://denoised-smoothing-tf/initial_model_resnet20\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: gs://denoised-smoothing-tf/initial_model_resnet20/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: gs://denoised-smoothing-tf/initial_model_resnet20/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLjm5gx4Dp1n"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwGi5W1tB6N9"
      },
      "source": [
        "# Optimizer and loss function.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=START_LR, momentum=0.9)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyuPYAniB8oo",
        "outputId": "79757349-0308-4672-c26d-b39b8b97eeb7"
      },
      "source": [
        "with strategy.scope():\n",
        "    rn_model = tf.keras.models.load_model(\"gs://denoised-smoothing-tf/initial_model_resnet20\")\n",
        "    rn_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "history = rn_model.fit(train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[lr_callback])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 22s 216ms/step - loss: 2.8476 - accuracy: 0.2661 - val_loss: 3.3992 - val_accuracy: 0.1212\n",
            "Epoch 2/200\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 2.2902 - accuracy: 0.4536 - val_loss: 3.9956 - val_accuracy: 0.1086\n",
            "Epoch 3/200\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 2.0309 - accuracy: 0.5288 - val_loss: 4.9698 - val_accuracy: 0.1036\n",
            "Epoch 4/200\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 1.8306 - accuracy: 0.5844 - val_loss: 4.1593 - val_accuracy: 0.1608\n",
            "Epoch 5/200\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 1.6509 - accuracy: 0.6316 - val_loss: 3.5013 - val_accuracy: 0.2088\n",
            "Epoch 6/200\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 1.5110 - accuracy: 0.6680 - val_loss: 3.4459 - val_accuracy: 0.2333\n",
            "Epoch 7/200\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 1.3964 - accuracy: 0.6935 - val_loss: 3.2565 - val_accuracy: 0.2720\n",
            "Epoch 8/200\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 1.2949 - accuracy: 0.7177 - val_loss: 2.0870 - val_accuracy: 0.4954\n",
            "Epoch 9/200\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 1.2001 - accuracy: 0.7413 - val_loss: 2.1167 - val_accuracy: 0.4553\n",
            "Epoch 10/200\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 1.1385 - accuracy: 0.7556 - val_loss: 2.1623 - val_accuracy: 0.4676\n",
            "Epoch 11/200\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 1.0657 - accuracy: 0.7722 - val_loss: 1.9061 - val_accuracy: 0.5237\n",
            "Epoch 12/200\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 1.0001 - accuracy: 0.7896 - val_loss: 1.3686 - val_accuracy: 0.6606\n",
            "Epoch 13/200\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.9681 - accuracy: 0.7904 - val_loss: 1.4594 - val_accuracy: 0.6108\n",
            "Epoch 14/200\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.9133 - accuracy: 0.8038 - val_loss: 1.4366 - val_accuracy: 0.6138\n",
            "Epoch 15/200\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.8903 - accuracy: 0.8051 - val_loss: 1.2940 - val_accuracy: 0.6742\n",
            "Epoch 16/200\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.8639 - accuracy: 0.8108 - val_loss: 1.5036 - val_accuracy: 0.6196\n",
            "Epoch 17/200\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.8333 - accuracy: 0.8157 - val_loss: 1.1977 - val_accuracy: 0.6739\n",
            "Epoch 18/200\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.8044 - accuracy: 0.8231 - val_loss: 1.1537 - val_accuracy: 0.7091\n",
            "Epoch 19/200\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.7817 - accuracy: 0.8271 - val_loss: 1.5094 - val_accuracy: 0.6247\n",
            "Epoch 20/200\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.7605 - accuracy: 0.8319 - val_loss: 1.0038 - val_accuracy: 0.7402\n",
            "Epoch 21/200\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.7625 - accuracy: 0.8276 - val_loss: 1.1869 - val_accuracy: 0.7092\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.7309 - accuracy: 0.8386 - val_loss: 1.1602 - val_accuracy: 0.6914\n",
            "Epoch 23/200\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.7052 - accuracy: 0.8423 - val_loss: 1.3175 - val_accuracy: 0.6418\n",
            "Epoch 24/200\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.6888 - accuracy: 0.8454 - val_loss: 1.1810 - val_accuracy: 0.6946\n",
            "Epoch 25/200\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.6735 - accuracy: 0.8514 - val_loss: 1.7180 - val_accuracy: 0.5899\n",
            "Epoch 26/200\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.6669 - accuracy: 0.8511 - val_loss: 1.2863 - val_accuracy: 0.6999\n",
            "Epoch 27/200\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.6612 - accuracy: 0.8538 - val_loss: 1.1254 - val_accuracy: 0.7231\n",
            "Epoch 28/200\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.6451 - accuracy: 0.8587 - val_loss: 1.0720 - val_accuracy: 0.7241\n",
            "Epoch 29/200\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.6387 - accuracy: 0.8596 - val_loss: 1.0588 - val_accuracy: 0.7319\n",
            "Epoch 30/200\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.6243 - accuracy: 0.8636 - val_loss: 1.5216 - val_accuracy: 0.6742\n",
            "Epoch 31/200\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.6153 - accuracy: 0.8649 - val_loss: 1.2193 - val_accuracy: 0.7172\n",
            "Epoch 32/200\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.6201 - accuracy: 0.8635 - val_loss: 1.2974 - val_accuracy: 0.7003\n",
            "Epoch 33/200\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.6009 - accuracy: 0.8692 - val_loss: 0.9739 - val_accuracy: 0.7631\n",
            "Epoch 34/200\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.5974 - accuracy: 0.8704 - val_loss: 1.2832 - val_accuracy: 0.6690\n",
            "Epoch 35/200\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.6004 - accuracy: 0.8686 - val_loss: 1.0136 - val_accuracy: 0.7413\n",
            "Epoch 36/200\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.5840 - accuracy: 0.8718 - val_loss: 0.8626 - val_accuracy: 0.7962\n",
            "Epoch 37/200\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.5804 - accuracy: 0.8740 - val_loss: 1.2897 - val_accuracy: 0.7002\n",
            "Epoch 38/200\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.5918 - accuracy: 0.8696 - val_loss: 1.2350 - val_accuracy: 0.7102\n",
            "Epoch 39/200\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.5678 - accuracy: 0.8777 - val_loss: 0.9496 - val_accuracy: 0.7690\n",
            "Epoch 40/200\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.5660 - accuracy: 0.8781 - val_loss: 1.4997 - val_accuracy: 0.6416\n",
            "Epoch 41/200\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.5627 - accuracy: 0.8788 - val_loss: 0.9625 - val_accuracy: 0.7846\n",
            "Epoch 42/200\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.5617 - accuracy: 0.8797 - val_loss: 0.8624 - val_accuracy: 0.7842\n",
            "Epoch 43/200\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.5532 - accuracy: 0.8821 - val_loss: 1.0173 - val_accuracy: 0.7497\n",
            "Epoch 44/200\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.5521 - accuracy: 0.8813 - val_loss: 0.9553 - val_accuracy: 0.7658\n",
            "Epoch 45/200\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.5505 - accuracy: 0.8819 - val_loss: 0.9822 - val_accuracy: 0.7616\n",
            "Epoch 46/200\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.5584 - accuracy: 0.8806 - val_loss: 0.8607 - val_accuracy: 0.7905\n",
            "Epoch 47/200\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 0.5321 - accuracy: 0.8887 - val_loss: 0.8429 - val_accuracy: 0.8040\n",
            "Epoch 48/200\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.5429 - accuracy: 0.8841 - val_loss: 1.4224 - val_accuracy: 0.7054\n",
            "Epoch 49/200\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.5368 - accuracy: 0.8881 - val_loss: 0.9212 - val_accuracy: 0.7545\n",
            "Epoch 50/200\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.6351 - accuracy: 0.8541 - val_loss: 0.6471 - val_accuracy: 0.8559\n",
            "Epoch 51/200\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 46ms/step - loss: 0.4547 - accuracy: 0.9193 - val_loss: 0.5557 - val_accuracy: 0.8814\n",
            "Epoch 52/200\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.4248 - accuracy: 0.9288 - val_loss: 0.5410 - val_accuracy: 0.8870\n",
            "Epoch 53/200\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.4027 - accuracy: 0.9364 - val_loss: 0.5532 - val_accuracy: 0.8825\n",
            "Epoch 54/200\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3934 - accuracy: 0.9391 - val_loss: 0.5950 - val_accuracy: 0.8699\n",
            "Epoch 55/200\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3877 - accuracy: 0.9401 - val_loss: 0.5521 - val_accuracy: 0.8822\n",
            "Epoch 56/200\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.3767 - accuracy: 0.9430 - val_loss: 0.5201 - val_accuracy: 0.8921\n",
            "Epoch 57/200\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3687 - accuracy: 0.9443 - val_loss: 0.5301 - val_accuracy: 0.8892\n",
            "Epoch 58/200\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.3618 - accuracy: 0.9463 - val_loss: 0.5613 - val_accuracy: 0.8788\n",
            "Epoch 59/200\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.3548 - accuracy: 0.9481 - val_loss: 0.5518 - val_accuracy: 0.8802\n",
            "Epoch 60/200\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.3508 - accuracy: 0.9498 - val_loss: 0.5542 - val_accuracy: 0.8790\n",
            "Epoch 61/200\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3502 - accuracy: 0.9492 - val_loss: 0.5121 - val_accuracy: 0.8923\n",
            "Epoch 62/200\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3381 - accuracy: 0.9526 - val_loss: 0.5241 - val_accuracy: 0.8895\n",
            "Epoch 63/200\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.3352 - accuracy: 0.9527 - val_loss: 0.5453 - val_accuracy: 0.8866\n",
            "Epoch 64/200\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.3374 - accuracy: 0.9506 - val_loss: 0.5259 - val_accuracy: 0.8897\n",
            "Epoch 65/200\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3275 - accuracy: 0.9552 - val_loss: 0.5240 - val_accuracy: 0.8893\n",
            "Epoch 66/200\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3217 - accuracy: 0.9553 - val_loss: 0.5362 - val_accuracy: 0.8860\n",
            "Epoch 67/200\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3169 - accuracy: 0.9564 - val_loss: 0.5249 - val_accuracy: 0.8918\n",
            "Epoch 68/200\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3143 - accuracy: 0.9566 - val_loss: 0.5227 - val_accuracy: 0.8900\n",
            "Epoch 69/200\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 49ms/step - loss: 0.3082 - accuracy: 0.9579 - val_loss: 0.5606 - val_accuracy: 0.8788\n",
            "Epoch 70/200\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.3053 - accuracy: 0.9594 - val_loss: 0.5517 - val_accuracy: 0.8787\n",
            "Epoch 71/200\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.3001 - accuracy: 0.9606 - val_loss: 0.5618 - val_accuracy: 0.8797\n",
            "Epoch 72/200\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2997 - accuracy: 0.9598 - val_loss: 0.5249 - val_accuracy: 0.8898\n",
            "Epoch 73/200\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.2954 - accuracy: 0.9594 - val_loss: 0.5725 - val_accuracy: 0.8777\n",
            "Epoch 74/200\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2956 - accuracy: 0.9603 - val_loss: 0.5553 - val_accuracy: 0.8805\n",
            "Epoch 75/200\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2875 - accuracy: 0.9643 - val_loss: 0.5510 - val_accuracy: 0.8793\n",
            "Epoch 76/200\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2831 - accuracy: 0.9630 - val_loss: 0.5053 - val_accuracy: 0.8906\n",
            "Epoch 77/200\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2805 - accuracy: 0.9652 - val_loss: 0.5094 - val_accuracy: 0.8891\n",
            "Epoch 78/200\n",
            "\n",
            "Epoch 00078: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2777 - accuracy: 0.9631 - val_loss: 0.5141 - val_accuracy: 0.8897\n",
            "Epoch 79/200\n",
            "\n",
            "Epoch 00079: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2765 - accuracy: 0.9630 - val_loss: 0.6032 - val_accuracy: 0.8698\n",
            "Epoch 80/200\n",
            "\n",
            "Epoch 00080: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 46ms/step - loss: 0.2734 - accuracy: 0.9641 - val_loss: 0.5191 - val_accuracy: 0.8883\n",
            "Epoch 81/200\n",
            "\n",
            "Epoch 00081: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.2749 - accuracy: 0.9626 - val_loss: 0.5391 - val_accuracy: 0.8855\n",
            "Epoch 82/200\n",
            "\n",
            "Epoch 00082: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2710 - accuracy: 0.9642 - val_loss: 0.4865 - val_accuracy: 0.8957\n",
            "Epoch 83/200\n",
            "\n",
            "Epoch 00083: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2649 - accuracy: 0.9663 - val_loss: 0.5619 - val_accuracy: 0.8821\n",
            "Epoch 84/200\n",
            "\n",
            "Epoch 00084: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2620 - accuracy: 0.9667 - val_loss: 0.5211 - val_accuracy: 0.8887\n",
            "Epoch 85/200\n",
            "\n",
            "Epoch 00085: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2604 - accuracy: 0.9668 - val_loss: 0.5357 - val_accuracy: 0.8851\n",
            "Epoch 86/200\n",
            "\n",
            "Epoch 00086: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2559 - accuracy: 0.9680 - val_loss: 0.5529 - val_accuracy: 0.8784\n",
            "Epoch 87/200\n",
            "\n",
            "Epoch 00087: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2550 - accuracy: 0.9672 - val_loss: 0.5059 - val_accuracy: 0.8893\n",
            "Epoch 88/200\n",
            "\n",
            "Epoch 00088: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2486 - accuracy: 0.9706 - val_loss: 0.6248 - val_accuracy: 0.8649\n",
            "Epoch 89/200\n",
            "\n",
            "Epoch 00089: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.2567 - accuracy: 0.9670 - val_loss: 0.5762 - val_accuracy: 0.8803\n",
            "Epoch 90/200\n",
            "\n",
            "Epoch 00090: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2471 - accuracy: 0.9703 - val_loss: 0.5235 - val_accuracy: 0.8873\n",
            "Epoch 91/200\n",
            "\n",
            "Epoch 00091: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2454 - accuracy: 0.9684 - val_loss: 0.5443 - val_accuracy: 0.8822\n",
            "Epoch 92/200\n",
            "\n",
            "Epoch 00092: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2458 - accuracy: 0.9687 - val_loss: 0.5414 - val_accuracy: 0.8843\n",
            "Epoch 93/200\n",
            "\n",
            "Epoch 00093: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2419 - accuracy: 0.9710 - val_loss: 0.5037 - val_accuracy: 0.8922\n",
            "Epoch 94/200\n",
            "\n",
            "Epoch 00094: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2395 - accuracy: 0.9701 - val_loss: 0.5296 - val_accuracy: 0.8864\n",
            "Epoch 95/200\n",
            "\n",
            "Epoch 00095: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2372 - accuracy: 0.9687 - val_loss: 0.5020 - val_accuracy: 0.8925\n",
            "Epoch 96/200\n",
            "\n",
            "Epoch 00096: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2359 - accuracy: 0.9711 - val_loss: 0.5465 - val_accuracy: 0.8838\n",
            "Epoch 97/200\n",
            "\n",
            "Epoch 00097: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2317 - accuracy: 0.9709 - val_loss: 0.5303 - val_accuracy: 0.8856\n",
            "Epoch 98/200\n",
            "\n",
            "Epoch 00098: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.2332 - accuracy: 0.9710 - val_loss: 0.5050 - val_accuracy: 0.8908\n",
            "Epoch 99/200\n",
            "\n",
            "Epoch 00099: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2308 - accuracy: 0.9717 - val_loss: 0.5210 - val_accuracy: 0.8864\n",
            "Epoch 100/200\n",
            "\n",
            "Epoch 00100: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2327 - accuracy: 0.9706 - val_loss: 0.4955 - val_accuracy: 0.8960\n",
            "Epoch 101/200\n",
            "\n",
            "Epoch 00101: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2196 - accuracy: 0.9758 - val_loss: 0.4712 - val_accuracy: 0.9011\n",
            "Epoch 102/200\n",
            "\n",
            "Epoch 00102: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2152 - accuracy: 0.9776 - val_loss: 0.4798 - val_accuracy: 0.8988\n",
            "Epoch 103/200\n",
            "\n",
            "Epoch 00103: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2105 - accuracy: 0.9786 - val_loss: 0.4767 - val_accuracy: 0.9006\n",
            "Epoch 104/200\n",
            "\n",
            "Epoch 00104: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2101 - accuracy: 0.9800 - val_loss: 0.4854 - val_accuracy: 0.8990\n",
            "Epoch 105/200\n",
            "\n",
            "Epoch 00105: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2084 - accuracy: 0.9801 - val_loss: 0.4765 - val_accuracy: 0.8984\n",
            "Epoch 106/200\n",
            "\n",
            "Epoch 00106: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.2072 - accuracy: 0.9806 - val_loss: 0.4801 - val_accuracy: 0.8986\n",
            "Epoch 107/200\n",
            "\n",
            "Epoch 00107: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2082 - accuracy: 0.9799 - val_loss: 0.4797 - val_accuracy: 0.8999\n",
            "Epoch 108/200\n",
            "\n",
            "Epoch 00108: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2020 - accuracy: 0.9826 - val_loss: 0.4774 - val_accuracy: 0.9002\n",
            "Epoch 109/200\n",
            "\n",
            "Epoch 00109: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2058 - accuracy: 0.9816 - val_loss: 0.4805 - val_accuracy: 0.9006\n",
            "Epoch 110/200\n",
            "\n",
            "Epoch 00110: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2045 - accuracy: 0.9811 - val_loss: 0.4795 - val_accuracy: 0.9003\n",
            "Epoch 111/200\n",
            "\n",
            "Epoch 00111: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2030 - accuracy: 0.9816 - val_loss: 0.4730 - val_accuracy: 0.8999\n",
            "Epoch 112/200\n",
            "\n",
            "Epoch 00112: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2040 - accuracy: 0.9820 - val_loss: 0.4746 - val_accuracy: 0.9016\n",
            "Epoch 113/200\n",
            "\n",
            "Epoch 00113: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2031 - accuracy: 0.9816 - val_loss: 0.4827 - val_accuracy: 0.9001\n",
            "Epoch 114/200\n",
            "\n",
            "Epoch 00114: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2004 - accuracy: 0.9829 - val_loss: 0.4720 - val_accuracy: 0.9039\n",
            "Epoch 115/200\n",
            "\n",
            "Epoch 00115: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.2012 - accuracy: 0.9817 - val_loss: 0.4779 - val_accuracy: 0.9015\n",
            "Epoch 116/200\n",
            "\n",
            "Epoch 00116: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2013 - accuracy: 0.9817 - val_loss: 0.4749 - val_accuracy: 0.9026\n",
            "Epoch 117/200\n",
            "\n",
            "Epoch 00117: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1981 - accuracy: 0.9835 - val_loss: 0.4785 - val_accuracy: 0.8998\n",
            "Epoch 118/200\n",
            "\n",
            "Epoch 00118: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.2010 - accuracy: 0.9825 - val_loss: 0.4821 - val_accuracy: 0.8991\n",
            "Epoch 119/200\n",
            "\n",
            "Epoch 00119: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1996 - accuracy: 0.9836 - val_loss: 0.4745 - val_accuracy: 0.9018\n",
            "Epoch 120/200\n",
            "\n",
            "Epoch 00120: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1981 - accuracy: 0.9842 - val_loss: 0.4778 - val_accuracy: 0.9026\n",
            "Epoch 121/200\n",
            "\n",
            "Epoch 00121: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.2005 - accuracy: 0.9821 - val_loss: 0.4737 - val_accuracy: 0.9020\n",
            "Epoch 122/200\n",
            "\n",
            "Epoch 00122: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 49ms/step - loss: 0.2000 - accuracy: 0.9825 - val_loss: 0.4699 - val_accuracy: 0.9019\n",
            "Epoch 123/200\n",
            "\n",
            "Epoch 00123: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.1992 - accuracy: 0.9825 - val_loss: 0.4799 - val_accuracy: 0.9007\n",
            "Epoch 124/200\n",
            "\n",
            "Epoch 00124: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1991 - accuracy: 0.9834 - val_loss: 0.4730 - val_accuracy: 0.9022\n",
            "Epoch 125/200\n",
            "\n",
            "Epoch 00125: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1985 - accuracy: 0.9830 - val_loss: 0.4791 - val_accuracy: 0.8988\n",
            "Epoch 126/200\n",
            "\n",
            "Epoch 00126: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1970 - accuracy: 0.9837 - val_loss: 0.4838 - val_accuracy: 0.9009\n",
            "Epoch 127/200\n",
            "\n",
            "Epoch 00127: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1952 - accuracy: 0.9836 - val_loss: 0.4899 - val_accuracy: 0.8977\n",
            "Epoch 128/200\n",
            "\n",
            "Epoch 00128: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1939 - accuracy: 0.9847 - val_loss: 0.4765 - val_accuracy: 0.9014\n",
            "Epoch 129/200\n",
            "\n",
            "Epoch 00129: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1972 - accuracy: 0.9835 - val_loss: 0.4781 - val_accuracy: 0.9005\n",
            "Epoch 130/200\n",
            "\n",
            "Epoch 00130: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1941 - accuracy: 0.9835 - val_loss: 0.4804 - val_accuracy: 0.9005\n",
            "Epoch 131/200\n",
            "\n",
            "Epoch 00131: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1955 - accuracy: 0.9833 - val_loss: 0.4761 - val_accuracy: 0.9008\n",
            "Epoch 132/200\n",
            "\n",
            "Epoch 00132: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.1969 - accuracy: 0.9835 - val_loss: 0.4649 - val_accuracy: 0.9035\n",
            "Epoch 133/200\n",
            "\n",
            "Epoch 00133: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1966 - accuracy: 0.9832 - val_loss: 0.4784 - val_accuracy: 0.9009\n",
            "Epoch 134/200\n",
            "\n",
            "Epoch 00134: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1937 - accuracy: 0.9846 - val_loss: 0.4884 - val_accuracy: 0.8986\n",
            "Epoch 135/200\n",
            "\n",
            "Epoch 00135: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1935 - accuracy: 0.9837 - val_loss: 0.4775 - val_accuracy: 0.8987\n",
            "Epoch 136/200\n",
            "\n",
            "Epoch 00136: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1929 - accuracy: 0.9845 - val_loss: 0.4775 - val_accuracy: 0.9010\n",
            "Epoch 137/200\n",
            "\n",
            "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1921 - accuracy: 0.9844 - val_loss: 0.4790 - val_accuracy: 0.8998\n",
            "Epoch 138/200\n",
            "\n",
            "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1932 - accuracy: 0.9837 - val_loss: 0.4896 - val_accuracy: 0.8972\n",
            "Epoch 139/200\n",
            "\n",
            "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1909 - accuracy: 0.9846 - val_loss: 0.4818 - val_accuracy: 0.8997\n",
            "Epoch 140/200\n",
            "\n",
            "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1915 - accuracy: 0.9844 - val_loss: 0.4672 - val_accuracy: 0.9035\n",
            "Epoch 141/200\n",
            "\n",
            "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.1901 - accuracy: 0.9844 - val_loss: 0.4832 - val_accuracy: 0.9006\n",
            "Epoch 142/200\n",
            "\n",
            "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1893 - accuracy: 0.9859 - val_loss: 0.4726 - val_accuracy: 0.9008\n",
            "Epoch 143/200\n",
            "\n",
            "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1918 - accuracy: 0.9849 - val_loss: 0.4770 - val_accuracy: 0.9003\n",
            "Epoch 144/200\n",
            "\n",
            "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1898 - accuracy: 0.9854 - val_loss: 0.4786 - val_accuracy: 0.8997\n",
            "Epoch 145/200\n",
            "\n",
            "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1883 - accuracy: 0.9856 - val_loss: 0.4815 - val_accuracy: 0.8996\n",
            "Epoch 146/200\n",
            "\n",
            "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1914 - accuracy: 0.9838 - val_loss: 0.4772 - val_accuracy: 0.9007\n",
            "Epoch 147/200\n",
            "\n",
            "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1879 - accuracy: 0.9858 - val_loss: 0.4791 - val_accuracy: 0.9005\n",
            "Epoch 148/200\n",
            "\n",
            "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1918 - accuracy: 0.9839 - val_loss: 0.4730 - val_accuracy: 0.9011\n",
            "Epoch 149/200\n",
            "\n",
            "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.1903 - accuracy: 0.9839 - val_loss: 0.4774 - val_accuracy: 0.9011\n",
            "Epoch 150/200\n",
            "\n",
            "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1916 - accuracy: 0.9833 - val_loss: 0.4779 - val_accuracy: 0.9009\n",
            "Epoch 151/200\n",
            "\n",
            "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1896 - accuracy: 0.9844 - val_loss: 0.4782 - val_accuracy: 0.9007\n",
            "Epoch 152/200\n",
            "\n",
            "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1897 - accuracy: 0.9847 - val_loss: 0.4792 - val_accuracy: 0.9008\n",
            "Epoch 153/200\n",
            "\n",
            "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1887 - accuracy: 0.9851 - val_loss: 0.4782 - val_accuracy: 0.9006\n",
            "Epoch 154/200\n",
            "\n",
            "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1873 - accuracy: 0.9854 - val_loss: 0.4774 - val_accuracy: 0.9014\n",
            "Epoch 155/200\n",
            "\n",
            "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1867 - accuracy: 0.9862 - val_loss: 0.4777 - val_accuracy: 0.9012\n",
            "Epoch 156/200\n",
            "\n",
            "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1867 - accuracy: 0.9862 - val_loss: 0.4769 - val_accuracy: 0.9016\n",
            "Epoch 157/200\n",
            "\n",
            "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1865 - accuracy: 0.9854 - val_loss: 0.4763 - val_accuracy: 0.9017\n",
            "Epoch 158/200\n",
            "\n",
            "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.1848 - accuracy: 0.9871 - val_loss: 0.4767 - val_accuracy: 0.9026\n",
            "Epoch 159/200\n",
            "\n",
            "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1875 - accuracy: 0.9852 - val_loss: 0.4763 - val_accuracy: 0.9027\n",
            "Epoch 160/200\n",
            "\n",
            "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1900 - accuracy: 0.9835 - val_loss: 0.4761 - val_accuracy: 0.9023\n",
            "Epoch 161/200\n",
            "\n",
            "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 46ms/step - loss: 0.1889 - accuracy: 0.9852 - val_loss: 0.4774 - val_accuracy: 0.9017\n",
            "Epoch 162/200\n",
            "\n",
            "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1861 - accuracy: 0.9859 - val_loss: 0.4769 - val_accuracy: 0.9013\n",
            "Epoch 163/200\n",
            "\n",
            "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 46ms/step - loss: 0.1859 - accuracy: 0.9861 - val_loss: 0.4763 - val_accuracy: 0.9011\n",
            "Epoch 164/200\n",
            "\n",
            "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1880 - accuracy: 0.9842 - val_loss: 0.4761 - val_accuracy: 0.9020\n",
            "Epoch 165/200\n",
            "\n",
            "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1868 - accuracy: 0.9854 - val_loss: 0.4773 - val_accuracy: 0.9015\n",
            "Epoch 166/200\n",
            "\n",
            "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.1859 - accuracy: 0.9860 - val_loss: 0.4779 - val_accuracy: 0.9011\n",
            "Epoch 167/200\n",
            "\n",
            "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 46ms/step - loss: 0.1880 - accuracy: 0.9848 - val_loss: 0.4777 - val_accuracy: 0.9011\n",
            "Epoch 168/200\n",
            "\n",
            "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1882 - accuracy: 0.9845 - val_loss: 0.4772 - val_accuracy: 0.9015\n",
            "Epoch 169/200\n",
            "\n",
            "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 55ms/step - loss: 0.1884 - accuracy: 0.9856 - val_loss: 0.4768 - val_accuracy: 0.9012\n",
            "Epoch 170/200\n",
            "\n",
            "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1869 - accuracy: 0.9855 - val_loss: 0.4770 - val_accuracy: 0.9015\n",
            "Epoch 171/200\n",
            "\n",
            "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1850 - accuracy: 0.9864 - val_loss: 0.4781 - val_accuracy: 0.9011\n",
            "Epoch 172/200\n",
            "\n",
            "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1880 - accuracy: 0.9860 - val_loss: 0.4764 - val_accuracy: 0.9020\n",
            "Epoch 173/200\n",
            "\n",
            "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1838 - accuracy: 0.9870 - val_loss: 0.4774 - val_accuracy: 0.9013\n",
            "Epoch 174/200\n",
            "\n",
            "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1862 - accuracy: 0.9861 - val_loss: 0.4770 - val_accuracy: 0.9016\n",
            "Epoch 175/200\n",
            "\n",
            "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1854 - accuracy: 0.9868 - val_loss: 0.4779 - val_accuracy: 0.9018\n",
            "Epoch 176/200\n",
            "\n",
            "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1870 - accuracy: 0.9857 - val_loss: 0.4789 - val_accuracy: 0.9009\n",
            "Epoch 177/200\n",
            "\n",
            "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1842 - accuracy: 0.9869 - val_loss: 0.4777 - val_accuracy: 0.9016\n",
            "Epoch 178/200\n",
            "\n",
            "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1881 - accuracy: 0.9845 - val_loss: 0.4765 - val_accuracy: 0.9021\n",
            "Epoch 179/200\n",
            "\n",
            "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1865 - accuracy: 0.9864 - val_loss: 0.4773 - val_accuracy: 0.9018\n",
            "Epoch 180/200\n",
            "\n",
            "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1859 - accuracy: 0.9865 - val_loss: 0.4764 - val_accuracy: 0.9013\n",
            "Epoch 181/200\n",
            "\n",
            "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1849 - accuracy: 0.9866 - val_loss: 0.4765 - val_accuracy: 0.9020\n",
            "Epoch 182/200\n",
            "\n",
            "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1866 - accuracy: 0.9861 - val_loss: 0.4772 - val_accuracy: 0.9026\n",
            "Epoch 183/200\n",
            "\n",
            "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1862 - accuracy: 0.9858 - val_loss: 0.4779 - val_accuracy: 0.9014\n",
            "Epoch 184/200\n",
            "\n",
            "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 0.1851 - accuracy: 0.9867 - val_loss: 0.4778 - val_accuracy: 0.9016\n",
            "Epoch 185/200\n",
            "\n",
            "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1869 - accuracy: 0.9856 - val_loss: 0.4776 - val_accuracy: 0.9015\n",
            "Epoch 186/200\n",
            "\n",
            "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1843 - accuracy: 0.9867 - val_loss: 0.4773 - val_accuracy: 0.9013\n",
            "Epoch 187/200\n",
            "\n",
            "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1863 - accuracy: 0.9858 - val_loss: 0.4775 - val_accuracy: 0.9013\n",
            "Epoch 188/200\n",
            "\n",
            "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1850 - accuracy: 0.9866 - val_loss: 0.4773 - val_accuracy: 0.9012\n",
            "Epoch 189/200\n",
            "\n",
            "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1863 - accuracy: 0.9861 - val_loss: 0.4773 - val_accuracy: 0.9013\n",
            "Epoch 190/200\n",
            "\n",
            "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1860 - accuracy: 0.9862 - val_loss: 0.4765 - val_accuracy: 0.9024\n",
            "Epoch 191/200\n",
            "\n",
            "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1878 - accuracy: 0.9857 - val_loss: 0.4772 - val_accuracy: 0.9013\n",
            "Epoch 192/200\n",
            "\n",
            "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1866 - accuracy: 0.9860 - val_loss: 0.4781 - val_accuracy: 0.9017\n",
            "Epoch 193/200\n",
            "\n",
            "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.1875 - accuracy: 0.9852 - val_loss: 0.4777 - val_accuracy: 0.9017\n",
            "Epoch 194/200\n",
            "\n",
            "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1868 - accuracy: 0.9859 - val_loss: 0.4773 - val_accuracy: 0.9013\n",
            "Epoch 195/200\n",
            "\n",
            "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1874 - accuracy: 0.9852 - val_loss: 0.4763 - val_accuracy: 0.9012\n",
            "Epoch 196/200\n",
            "\n",
            "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1862 - accuracy: 0.9856 - val_loss: 0.4767 - val_accuracy: 0.9014\n",
            "Epoch 197/200\n",
            "\n",
            "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1841 - accuracy: 0.9864 - val_loss: 0.4770 - val_accuracy: 0.9019\n",
            "Epoch 198/200\n",
            "\n",
            "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1900 - accuracy: 0.9839 - val_loss: 0.4778 - val_accuracy: 0.9013\n",
            "Epoch 199/200\n",
            "\n",
            "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.1872 - accuracy: 0.9851 - val_loss: 0.4761 - val_accuracy: 0.9016\n",
            "Epoch 200/200\n",
            "\n",
            "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "49/49 [==============================] - 3s 47ms/step - loss: 0.1854 - accuracy: 0.9865 - val_loss: 0.4748 - val_accuracy: 0.9014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "v1JWLrxYDFZ3",
        "outputId": "c1e0b2ed-ef98-421d-fca7-cf17adf4d7ee"
      },
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"test loss\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "rn_model.save(\"gs://denoised-smoothing-tf/resnet20_classifier\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: gs://denoised-smoothing-tf/resnet20_classifier/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: gs://denoised-smoothing-tf/resnet20_classifier/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6BohaLaEroC",
        "outputId": "d3e5c6bf-38d3-4c43-cd00-add46371b8bb"
      },
      "source": [
        "with strategy.scope():\n",
        "    _, train_acc = rn_model.evaluate(train_ds, verbose=0)\n",
        "    _, test_acc = rn_model.evaluate(test_ds, verbose=0)\n",
        "\n",
        "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
        "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 99.06%\n",
            "Test accuracy: 90.14%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}